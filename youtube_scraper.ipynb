{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Scraper for Video Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Set Up API Key\n",
    "> Import necessary libraries and set up the YouTube API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time  # Import the time module for rate limiting\n",
    "from dotenv import load_dotenv # import .env file for API key\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up YouTube API key\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters and Functions\n",
    "> Define categories, maximum results, minimum samples per category, and functions for sanitizing descriptions and scraping YouTube data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories and other parameters\n",
    "categories = ['Travel Blogs', 'Science and Technology', 'Food', 'Manufacturing', 'History', 'Art and Music']\n",
    "max_results = 50\n",
    "min_samples_per_category = 1700\n",
    "\n",
    "# Initialize an empty list to store all videos\n",
    "all_videos = []\n",
    "\n",
    "# Function to sanitize video descriptions\n",
    "def sanitize_description(description):\n",
    "    # Use regular expressions to remove unwanted information\n",
    "    # For example, removing email addresses, phone numbers, etc.\n",
    "    # Modify this based on your specific needs\n",
    "    cleaned_description = re.sub(r'\\S+@\\S+', '', description)\n",
    "    return cleaned_description\n",
    "\n",
    "# Function to scrape YouTube data for a given category\n",
    "def scrape_youtube_data(category):\n",
    "    videos = []\n",
    "    while len(videos) < min_samples_per_category:\n",
    "        for query in [f'{category} videos', f'{category} documentary']:\n",
    "            try:\n",
    "                # Make a YouTube API search request\n",
    "                request = youtube.search().list(\n",
    "                    q=query,\n",
    "                    part='id,snippet',\n",
    "                    type='video',\n",
    "                    maxResults=max_results\n",
    "                )\n",
    "                response = request.execute()\n",
    "                items = response.get('items', [])\n",
    "\n",
    "                # Loop through the retrieved videos\n",
    "                for item in items:\n",
    "                    video_id = item['id']['videoId']\n",
    "                    title = item['snippet']['title']\n",
    "\n",
    "                    # Make a YouTube API video request\n",
    "                    video_request = youtube.videos().list(\n",
    "                        part='snippet',\n",
    "                        id=video_id\n",
    "                    )\n",
    "                    video_response = video_request.execute()\n",
    "                    description = video_response['items'][0]['snippet']['description']\n",
    "\n",
    "                    # Sanitize the description using the defined function\n",
    "                    cleaned_description = sanitize_description(description)\n",
    "\n",
    "                    # Append video information to the list\n",
    "                    videos.append({'Video id': video_id, 'Title': title, 'Description': cleaned_description, 'Category': category})\n",
    "            except Exception as e:\n",
    "                print(f\"Error in API request: {str(e)}\")\n",
    "\n",
    "            # Add rate limiting to avoid hitting API rate limits\n",
    "            time.sleep(1)  # Sleep for 1 second between requests\n",
    "\n",
    "        # Break the loop if there are no more results\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape YouTube Data\n",
    "> Loop through each category and scrape YouTube data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each category and scrape YouTube data\n",
    "for category in categories:\n",
    "    videos = scrape_youtube_data(category)\n",
    "    all_videos.extend(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame and Save to CSV\n",
    "> Create a DataFrame and save the data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame and CSV file\n",
    "df = pd.DataFrame(all_videos)\n",
    "df.to_csv('youtube_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The YouTube data has been successfully scraped and stored in a CSV file (`youtube_data.csv`). This dataset can be used for text classification tasks based on video descriptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
